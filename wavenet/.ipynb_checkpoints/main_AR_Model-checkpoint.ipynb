{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import torchsummary\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conditional_WaveNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                layer_size=10,\n",
    "                stack_size=1,\n",
    "                residual_channels = 64,\n",
    "                skip_connections = 128,\n",
    "                input_chan = 8,\n",
    "                condition_channels=12,\n",
    "                out_chan = 8,\n",
    "                bias=False):\n",
    "        super().__init__()\n",
    "        #Hyper parameters\n",
    "        self.input_chan = input_chan\n",
    "        self.out_chan = out_chan\n",
    "        self.condition_channels = condition_channels\n",
    "\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_connections = skip_connections\n",
    "\n",
    "        #Build Model\n",
    "        self.receptive_fields = self.calc_receptive_fields(layer_size, stack_size)\n",
    "\n",
    "        self.dilations = []\n",
    "        self.dilated_queues = []\n",
    "\n",
    "\n",
    "        self.causal_conv = CausalConv1d(in_channels=input_chan, out_channels=residual_channels)\n",
    "        self.res_stack = ResidualStack(layer_size=layer_size, stack_size=stack_size, condition_channels=self.condition_channels,\n",
    "                                    res_channels=residual_channels, skip_channels=skip_connections)\n",
    "        #self.last_net = LastNet(128, 256)\n",
    "        self.last_nets = nn.ModuleList(LastNet(self.skip_connections, 256) for _ in range(self.out_chan))\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_receptive_fields(layer_size, stack_size):\n",
    "        layers = [2 ** i for i in range(layer_size)] * stack_size\n",
    "        num_receptive_fields = np.sum(np.unique(np.array(layers)))\n",
    "        \n",
    "        return int(num_receptive_fields)\n",
    "    \n",
    "    def calc_output_size(self, x):\n",
    "        output_size = int(x.size(2)) - self.receptive_fields\n",
    "        self.check_input_size(x, output_size)\n",
    "\n",
    "        return output_size\n",
    "    \n",
    "    def check_input_size(self, x, output_size):\n",
    "        if output_size < 1:\n",
    "            raise NameError('The data x is too short! The expected output size is {}'.format(output_size))\n",
    "    \n",
    "    def forward(self, x, condition):\n",
    "        \"\"\"\n",
    "        The size of timestep(3rd dimention) has to be bigger than receptive fields\n",
    "        :param x: Tensor[batch, timestep, channels]\n",
    "        :return: Tensor[batch, timestep, channels]\n",
    "        \"\"\"\n",
    "        if len(x.size()) > 3:\n",
    "            x = x.squeeze(1)\n",
    "        output = x.transpose(1,2).contiguous()\n",
    "        output_size = self.calc_output_size(output)\n",
    "        \n",
    "        condition = condition.transpose(1,2).contiguous()\n",
    "        \n",
    "        output = self.causal_conv(output)\n",
    "        #print(\"causal_conv output shape: {}, condition shape : {}\".format(output.shape, condition.shape))\n",
    "        \n",
    "        skips = self.res_stack(output, condition, output_size)\n",
    "        \n",
    "        output = torch.sum(skips, dim=0)\n",
    "        #output = self.last_net(output)\n",
    "        decisions  = self.last_nets[0](output).unsqueeze(1)\n",
    "        \n",
    "        for i in range(1, len(self.last_nets)):\n",
    "            o = self.last_nets[i](output).unsqueeze(1)\n",
    "            decisions = torch.cat([decisions, o], axis=1)\n",
    "\n",
    "        return decisions.transpose(1, 2).contiguous().transpose(2,3).contiguous()\n",
    "\n",
    "#%%\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # To match size of input and output, set padding = 1\n",
    "        self.conv = nn.Conv1d(in_channels=in_channels, out_channels=out_channels,\n",
    "                            padding=1, stride=1, kernel_size=2, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #To make size of output same as size of input, remove last value\n",
    "        x = self.conv(x)[:,:,:-1] #(minibatch, channels, length) \n",
    "        return x\n",
    "\n",
    "#%%\n",
    "class DilatedConv1d(nn.Module):\n",
    "    def __init__(self, channels, dilation=1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv1d(in_channels=channels, out_channels=channels,\n",
    "                            kernel_size=2, stride=1,\n",
    "                            dilation=dilation, padding=0,\n",
    "                            bias=bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.conv(x)\n",
    "        return output\n",
    "\n",
    "#%%\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, res_channels, skip_channels, cond_channels=12, dilation=1, bias=False):\n",
    "        super().__init__()\n",
    "        self.causal_gate = DilatedConv1d(channels=res_channels, dilation=dilation, bias=bias)\n",
    "        self.causal_filter = DilatedConv1d(channels=res_channels, dilation=dilation, bias=bias)\n",
    "        \n",
    "        self.condition_gate = nn.Conv1d(in_channels=cond_channels, out_channels=res_channels, kernel_size=1, stride=1)\n",
    "        self.condition_filter = nn.Conv1d(in_channels=cond_channels, out_channels=res_channels, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.conv_res = nn.Conv1d(in_channels=res_channels, out_channels=res_channels, padding=0, stride=1, kernel_size=1)\n",
    "        self.conv_skip = nn.Conv1d(in_channels=res_channels, out_channels=skip_channels, padding=0, stride=1, kernel_size=1)\n",
    "\n",
    "        self.gated_tanh = nn.Tanh()\n",
    "        self.gated_sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, c, skip_size):\n",
    "        \"\"\"\n",
    "        :param x: Sound data which is predicted previously\n",
    "        :param c: Conditional inputs => acceleration data\n",
    "        :param skip_size: The last output size for loss and prediction\n",
    "        \"\"\"\n",
    "        #print(\"causal gate shape : {}\".format(self.causal_gate(x).shape))\n",
    "        #print(\"condition gate shape : {}\".format(self.condition_gate(c).shape))\n",
    "        \n",
    "        conv_gate   = self.causal_gate(x)\n",
    "        conv_gate  += self.condition_gate(c)[:,:,-conv_gate.size(2):]\n",
    "        \n",
    "        conv_filter = self.causal_filter(x)\n",
    "        conv_filter += self.condition_filter(c)[:,:,-conv_filter.size(2):]\n",
    "        \n",
    "        out = self.gated_tanh(conv_filter) * self.gated_sig(conv_gate)\n",
    "        \n",
    "        skip = self.conv_skip(out)\n",
    "        skip = skip[:,:,-skip_size:]\n",
    "        \n",
    "        # forward for residual conv\n",
    "        input_cut = x[:,:,-out.size(2):]\n",
    "        res = self.conv_res(out) + input_cut\n",
    "\n",
    "        return res, skip\n",
    "\n",
    "#%%\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, layer_size=10, stack_size=5, res_channels=32, skip_channels=1, condition_channels=12):\n",
    "        \"\"\"\n",
    "        Stack ResidualBlock by layer and stack size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "        self.stack_size = stack_size\n",
    "        self.res_channels = res_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.condition_channels=condition_channels\n",
    "\n",
    "        self.res_blocks = self.stack_res_block(res_channels, skip_channels, condition_channels)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _residual_block(res_channels, skip_channels, dilation, conditional_channels):\n",
    "        block = ResidualBlock(res_channels=res_channels, skip_channels=skip_channels, dilation=dilation, cond_channels=conditional_channels)\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            block = torch.nn.DataParallel(block)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            block.cuda()\n",
    "        \n",
    "        return block\n",
    "\n",
    "    def stack_res_block(self, res_channels, skip_channels, condition_channels):\n",
    "        \"\"\"\n",
    "        Prepare dilated convolution blocks by layer and stack size\n",
    "        \"\"\"\n",
    "        res_blocks = []\n",
    "        dilations = self.build_dilations()\n",
    "\n",
    "        for dilation in dilations:\n",
    "            block = self._residual_block(res_channels, skip_channels, dilation, condition_channels)\n",
    "            res_blocks.append(block)\n",
    "        \n",
    "        return res_blocks\n",
    "            \n",
    "    \n",
    "    def build_dilations(self):\n",
    "        dilations = []\n",
    "        # 5 = stack[layer 1, layer 2, layer 3, layer 4, layer 5]\n",
    "        for s in range(self.stack_size):\n",
    "            # 10 = layer[dilation=1, dilation=2,4,8,16,32,64,128,256,512]\n",
    "            for l in range(self.layer_size):\n",
    "                dilations.append(2 ** l)\n",
    "        \n",
    "        return dilations\n",
    "    \n",
    "    def forward(self, x, condition, skip_size):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param condition: Condition data => Acceleration data\n",
    "        :param skip_size: The last output size for loss and prediction\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        output = x\n",
    "        output_size = output.size(2)\n",
    "        skip_connections = []\n",
    "\n",
    "        for res_block in self.res_blocks:\n",
    "            #output is the next input\n",
    "            #print(\"output size: {} skip_size : {}\".format(output_size, skip_size))\n",
    "            #print(\"condition shape : {} output shape : {}\".format(condition[:,:,-output_size:].shape, output.shape))\n",
    "            output, skip = res_block(output, condition[:,:,-output_size:], skip_size)\n",
    "            #print(\"resblock out shape : {} skip shape : {} condition shape {}\".format(output.shape, skip.shape, condition[:,:,:-output_size].shape))\n",
    "            skip_connections.append(skip)\n",
    "            output_size = output.size(2)\n",
    "        \n",
    "        return torch.stack(skip_connections)\n",
    "\n",
    "#%%\n",
    "class LastNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Last network of the wavenet\n",
    "        :param channels: number of channels for input and output\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 512, 1)\n",
    "        self.conv2 = nn.Conv1d(512, out_channels, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.relu(x)\n",
    "        output = self.conv1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.conv2(output)\n",
    "        #output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/data/datasets/hyundai\"\n",
    "ACC_PICKLE = os.path.join(PATH, \"stationary_accel_data.pickle\")\n",
    "SND_PICKLE = os.path.join(PATH, \"stationary_sound_data.pickle\")\n",
    "\n",
    "WINDOW_SIZE = 8192\n",
    "HOP_LEN = 256\n",
    "\n",
    "#with open(ACC_PICKLE, \"rb\") as f:\n",
    "#    acc_list = pickle.load(f)\n",
    "\n",
    "#with open(SND_PICKLE, \"rb\") as f:\n",
    "#    snd_list = pickle.load(f)\n",
    "\n",
    "acc_data = np.load(os.path.join(PATH, \"stationary_acc_win_1500_hop_256.npy\"))\n",
    "snd_data = np.load(os.path.join(PATH, \"stationary_snd_win_1500_hop_256.npy\"))\n",
    "\n",
    "print(\"DATA LOADED ACC : {} SND : {}\".format(acc_data.shape, snd_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mu_law_encoder(nn.Module):\n",
    "    def __init__(self, quantization_channels=256, rescale_factor=100):\n",
    "        super().__init__()\n",
    "        self.encoder = torchaudio.transforms.MuLawEncoding(quantization_channels=quantization_channels)\n",
    "        self.rescale_factor = rescale_factor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x / self.rescale_factor\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "class mu_law_decoder(nn.Module):\n",
    "    def init(self, quantization_channels=256, rescale_factor=100):\n",
    "        super().__init__()\n",
    "        self.quantization_channels = quantization_channels\n",
    "        self.rescale_factor = rescale_factor\n",
    "        self.decoder = torchaudio.transforms.MuLawDecoding(quantization_channels=quantization_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = x * self.rescale_factor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wavenet_Dataset(torch.utils.data.Dataset): \n",
    "  def __init__(self, x, y, transform=None):\n",
    "    self.x_data = x\n",
    "    self.y_data = y\n",
    "    \n",
    "    print(\"x shape : {}  y shape : {}\".format(self.x_data.shape, self.y_data.shape))\n",
    "    \n",
    "    self.transform = transform\n",
    "    \n",
    "    self.normalizer = torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "\n",
    "  def __len__(self): \n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx): \n",
    "    x = self.x_data[idx, :, :]\n",
    "    y = self.y_data[idx, :, :]\n",
    "\n",
    "    if self.transform is not None:\n",
    "        x = self.transform(x).float()\n",
    "        y = self.transform(y)\n",
    "    \n",
    "    #x = self.normalizer(x) #normalize\n",
    "    x /= 255.\n",
    "    #y /= 255.\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 16\n",
    "EPOCH = 30\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "                                    torchvision.transforms.ToTensor(),\n",
    "                                    mu_law_encoder()\n",
    "                                    ])\n",
    "receptive_field = 1500 - 1474\n",
    "dataset = Wavenet_Dataset(x=acc_data, y=snd_data, receptive_field=receptive_field, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conditional_WaveNet()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "torchsummary.summary(model, (27, 12))\n",
    "\n",
    "#Define Loss and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit36598a2dc6be465b894ec45159f939a2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
